{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0fa9ee8-ea57-4e11-9aa3-ea4376cdc884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a55e395-f164-4c46-bfac-11e3c05711f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d5f2d22-17d2-4d79-b8a4-fa79416d8f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "677314a0-73f2-4e3b-9d50-200eed2c29d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b54305d-d51f-40a6-b887-0cc19c4192a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "463088ea-d963-4177-aacd-2226eb5aa701",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05190ac8-8243-4980-9642-94e9bfdb38b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"What is the difference between llama 4 and llama 3.3 models? I am planning to run the models in local. I have NVIDIA RTX Geoforce 4080. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48233555-5d98-4f03-88e6-a0216904eeb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As of my last knowledge update in October 2023, specific details about the differences between LLaMA 4 and LLaMA 3.3 models were not widely discussed, as LLaMA 4 may not have been officially released or detailed publicly. However, I can provide some general considerations that might typically differentiate versions of machine learning models, particularly language models like those in the LLaMA series:\n",
      "\n",
      "1. **Model Architecture**: Newer versions might implement more advanced or optimized architectures that can improve performance or efficiency, allowing for better understanding and generation of text.\n",
      "\n",
      "2. **Training Data**: LLaMA 4 could potentially be trained on a larger or more diverse dataset compared to LLaMA 3.3, which may lead to improved generalization and better handling of various topics and languages.\n",
      "\n",
      "3. **Performance**: Newer models generally aim to improve upon the previous versions in terms of both speed and accuracy. This could manifest as faster inference times or reduced likelihood of generating irrelevant or nonsensical text.\n",
      "\n",
      "4. **Parameter Size**: LLaMA 4 may include models with differing parameter sizes, potentially offering a larger capacity compared to its predecessors, which would allow for more complex understanding and generation.\n",
      "\n",
      "5. **Fine-Tuning**: Additional fine-tuning options or techniques could be introduced in newer versions, enhancing the adaptability of the model for specific tasks or domains.\n",
      "\n",
      "6. **Compatibility and Ease of Use**: Updates may also bring improvements in terms of frameworks or APIs used for integration, making them easier to deploy or utilize locally.\n",
      "\n",
      "7. **Efficiency**: Newer models might include optimizations for memory usage or computational efficiency, which can be particularly relevant when running models on consumer-grade hardware like an NVIDIA RTX 4080.\n",
      "\n",
      "If you are planning to run these models locally, ensure that you have adequate hardware capabilities. The NVIDIA RTX 4080 is quite powerful and should handle many deep learning tasks effectively, though you might still need to consider memory limits depending on the model sizes you intend to use.\n",
      "\n",
      "For the most accurate information, especially specific differences between LLaMA 4 and LLaMA 3.3, I would recommend checking the official documentation, release notes, or research papers associated with these models if they are available.\n"
     ]
    }
   ],
   "source": [
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"user\", \"content\":message}])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cc8f01-f34d-4586-88ef-1ba5436bb09d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
